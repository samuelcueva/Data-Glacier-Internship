{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "Data_Ingestion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuelcueva/Data-Glacier-Internship/blob/master/week%206/Data_Ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "209db74f"
      },
      "source": [
        "# Data Ingestion"
      ],
      "id": "209db74f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abc6db0"
      },
      "source": [
        "## Dependencies"
      ],
      "id": "9abc6db0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8rWd7_PBNq0",
        "outputId": "42f0b1cf-d422-4333-bc0b-f2d66ca9933e"
      },
      "source": [
        "!python -m pip install \"dask[complete]\""
      ],
      "id": "k8rWd7_PBNq0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.3.0)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.1.5)\n",
            "Collecting distributed>=2.0\n",
            "  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bokeh>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2.3.3)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 76.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYaml in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (3.13)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (7.1.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (21.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.8.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (5.1.1)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.11.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (7.1.2)\n",
            "Collecting distributed>=2.0\n",
            "  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 83.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.0.2)\n",
            "Collecting cloudpickle>=0.2.1\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.0.0)\n",
            "Collecting distributed>=2.0\n",
            "  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 61.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 64.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (57.2.0)\n",
            "  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 30.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n",
            "\u001b[K     |████████████████████████████████| 715 kB 70.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n",
            "\u001b[K     |████████████████████████████████| 705 kB 65.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n",
            "\u001b[K     |████████████████████████████████| 699 kB 59.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 59.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n",
            "\u001b[K     |████████████████████████████████| 684 kB 63.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 56.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 55.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 54.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n",
            "\u001b[K     |████████████████████████████████| 672 kB 51.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 55.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 54.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n",
            "\u001b[K     |████████████████████████████████| 656 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (5.4.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=1.0.0->dask[complete]) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh>=1.0.0->dask[complete]) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[complete]) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh>=1.0.0->dask[complete]) (1.15.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0->dask[complete]) (1.0.1)\n",
            "Installing collected packages: locket, cloudpickle, partd, fsspec, distributed\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "Successfully installed cloudpickle-1.6.0 distributed-2.30.1 fsspec-2021.7.0 locket-0.2.1 partd-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b87724d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "import os"
      ],
      "id": "7b87724d",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAAqNpMS1qg1"
      },
      "source": [
        "## Data\n",
        "* Dataset(Kaggle) : https://www.kaggle.com/kentonnlp/2014-new-york-city-taxi-trips?select=nyc_taxi_data_2014.csv\n",
        "* The Dataset was download on my Google Drive to be used in this Colab Notebook\n",
        "* Shared file on Drive: https://drive.google.com/file/d/12HilR722L7wiL4T1iuB83oBlMzrKOSbh/view?usp=sharing\n"
      ],
      "id": "eAAqNpMS1qg1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdIvayyn5gC_",
        "outputId": "22a32743-2ab5-4a1a-8d76-9b8f2301b8d6"
      },
      "source": [
        "# to use file from Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BdIvayyn5gC_",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVOkx_rk7ui2",
        "outputId": "d212a95c-3ec9-4f6a-a50b-176df2132034"
      },
      "source": [
        "# To extract the zio file\n",
        "!unzip  /content/drive/MyDrive/nyc_taxi_data_2014.csv.zip"
      ],
      "id": "qVOkx_rk7ui2",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/nyc_taxi_data_2014.csv.zip\n",
            "  inflating: nyc_taxi_data_2014.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9bb0127",
        "outputId": "55f4371a-4afa-40a8-e3f6-d29360f4649f"
      },
      "source": [
        "!ls -lh"
      ],
      "id": "b9bb0127",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2.4G\n",
            "drwx------ 5 root root 4.0K Aug  9 04:35 drive\n",
            "-rw-r--r-- 1 root root 2.4G Sep 20  2019 nyc_taxi_data_2014.csv\n",
            "drwxr-xr-x 1 root root 4.0K Jul 16 13:20 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97be74c1"
      },
      "source": [
        "file weigtht: 2.4G "
      ],
      "id": "97be74c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bea62cbc"
      },
      "source": [
        "## Testing reading options "
      ],
      "id": "bea62cbc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9547de78"
      },
      "source": [
        "### Using Pandas"
      ],
      "id": "9547de78"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9a5b76c"
      },
      "source": [
        "file_path = 'nyc_taxi_data_2014.csv'"
      ],
      "id": "d9a5b76c",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af1380b2",
        "outputId": "93d9af1c-e4e0-4b89-8d62-f83f82253c02"
      },
      "source": [
        "%%time\n",
        "df=pd.read_csv(file_path)\n",
        "df"
      ],
      "id": "af1380b2",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:2: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 25.8 s, sys: 2.68 s, total: 28.5 s\n",
            "Wall time: 32.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwhF4_sQD4QZ",
        "outputId": "2b1b995b-baf5-410e-8d09-f00aade79a9f"
      },
      "source": [
        "df.info()"
      ],
      "id": "mwhF4_sQD4QZ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14999999 entries, 0 to 14999998\n",
            "Data columns (total 18 columns):\n",
            " #   Column              Dtype  \n",
            "---  ------              -----  \n",
            " 0   vendor_id           object \n",
            " 1   pickup_datetime     object \n",
            " 2   dropoff_datetime    object \n",
            " 3   passenger_count     int64  \n",
            " 4   trip_distance       float64\n",
            " 5   pickup_longitude    float64\n",
            " 6   pickup_latitude     float64\n",
            " 7   rate_code           int64  \n",
            " 8   store_and_fwd_flag  object \n",
            " 9   dropoff_longitude   float64\n",
            " 10  dropoff_latitude    float64\n",
            " 11  payment_type        object \n",
            " 12  fare_amount         float64\n",
            " 13  surcharge           float64\n",
            " 14  mta_tax             float64\n",
            " 15  tip_amount          float64\n",
            " 16  tolls_amount        float64\n",
            " 17  total_amount        float64\n",
            "dtypes: float64(11), int64(2), object(5)\n",
            "memory usage: 2.0+ GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRowaVDDGs0q"
      },
      "source": [
        "Using Pandas my local machine reduces its performance because pd.read_csv loads the whole file into memory and my local memory is only 6GB"
      ],
      "id": "wRowaVDDGs0q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b013a65"
      },
      "source": [
        "### Using Dask"
      ],
      "id": "6b013a65"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV9mixgMEegd"
      },
      "source": [
        "Dask first creates a task graph "
      ],
      "id": "IV9mixgMEegd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "be691839",
        "outputId": "2cc761c0-fe87-4f5c-97d1-2135cbf5e1bb"
      },
      "source": [
        "df=dd.read_csv(file_path,blocksize=250e6)\n",
        "df"
      ],
      "id": "be691839",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div><strong>Dask DataFrame Structure:</strong></div>\n",
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vendor_id</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>dropoff_datetime</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>trip_distance</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>rate_code</th>\n",
              "      <th>store_and_fwd_flag</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>payment_type</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>surcharge</th>\n",
              "      <th>mta_tax</th>\n",
              "      <th>tip_amount</th>\n",
              "      <th>tolls_amount</th>\n",
              "      <th>total_amount</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>npartitions=11</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "<div>Dask Name: from-delayed, 33 tasks</div>"
            ],
            "text/plain": [
              "Dask DataFrame Structure:\n",
              "               vendor_id pickup_datetime dropoff_datetime passenger_count trip_distance pickup_longitude pickup_latitude rate_code store_and_fwd_flag dropoff_longitude dropoff_latitude payment_type fare_amount surcharge  mta_tax tip_amount tolls_amount total_amount\n",
              "npartitions=11                                                                                                                                                                                                                                                           \n",
              "                  object          object           object           int64       float64          float64         float64     int64             object           float64          float64       object     float64   float64  float64    float64      float64      float64\n",
              "                     ...             ...              ...             ...           ...              ...             ...       ...                ...               ...              ...          ...         ...       ...      ...        ...          ...          ...\n",
              "...                  ...             ...              ...             ...           ...              ...             ...       ...                ...               ...              ...          ...         ...       ...      ...        ...          ...          ...\n",
              "                     ...             ...              ...             ...           ...              ...             ...       ...                ...               ...              ...          ...         ...       ...      ...        ...          ...          ...\n",
              "                     ...             ...              ...             ...           ...              ...             ...       ...                ...               ...              ...          ...         ...       ...      ...        ...          ...          ...\n",
              "Dask Name: from-delayed, 33 tasks"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmujRfuXGAPl"
      },
      "source": [
        "Now, it's executed"
      ],
      "id": "RmujRfuXGAPl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta97Ibi6IAiF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "outputId": "d0656c91-5903-4b15-ecae-c8307d3ce995"
      },
      "source": [
        "%%time \n",
        "df.compute()"
      ],
      "id": "ta97Ibi6IAiF",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dask/core.py:121: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  return func(*(_execute_task(a, cache) for a in args))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 31.1 s, sys: 1.59 s, total: 32.7 s\n",
            "Wall time: 1min 4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vendor_id</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>dropoff_datetime</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>trip_distance</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>rate_code</th>\n",
              "      <th>store_and_fwd_flag</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>payment_type</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>surcharge</th>\n",
              "      <th>mta_tax</th>\n",
              "      <th>tip_amount</th>\n",
              "      <th>tolls_amount</th>\n",
              "      <th>total_amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CMT</td>\n",
              "      <td>2014-01-09 20:45:25</td>\n",
              "      <td>2014-01-09 20:52:31</td>\n",
              "      <td>1</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-73.994770</td>\n",
              "      <td>40.736828</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>-73.982227</td>\n",
              "      <td>40.731790</td>\n",
              "      <td>CRD</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CMT</td>\n",
              "      <td>2014-01-09 20:46:12</td>\n",
              "      <td>2014-01-09 20:55:12</td>\n",
              "      <td>1</td>\n",
              "      <td>1.40</td>\n",
              "      <td>-73.982392</td>\n",
              "      <td>40.773382</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>-73.960449</td>\n",
              "      <td>40.763995</td>\n",
              "      <td>CRD</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CMT</td>\n",
              "      <td>2014-01-09 20:44:47</td>\n",
              "      <td>2014-01-09 20:59:46</td>\n",
              "      <td>2</td>\n",
              "      <td>2.30</td>\n",
              "      <td>-73.988570</td>\n",
              "      <td>40.739406</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>-73.986626</td>\n",
              "      <td>40.765217</td>\n",
              "      <td>CRD</td>\n",
              "      <td>11.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>14.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CMT</td>\n",
              "      <td>2014-01-09 20:44:57</td>\n",
              "      <td>2014-01-09 20:51:40</td>\n",
              "      <td>1</td>\n",
              "      <td>1.70</td>\n",
              "      <td>-73.960213</td>\n",
              "      <td>40.770464</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>-73.979863</td>\n",
              "      <td>40.777050</td>\n",
              "      <td>CRD</td>\n",
              "      <td>7.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CMT</td>\n",
              "      <td>2014-01-09 20:47:09</td>\n",
              "      <td>2014-01-09 20:53:32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.90</td>\n",
              "      <td>-73.995371</td>\n",
              "      <td>40.717248</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>-73.984367</td>\n",
              "      <td>40.720524</td>\n",
              "      <td>CRD</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186563</th>\n",
              "      <td>VTS</td>\n",
              "      <td>2014-02-03 19:28:00</td>\n",
              "      <td>2014-02-03 19:37:00</td>\n",
              "      <td>5</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-73.991223</td>\n",
              "      <td>40.718037</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-74.000675</td>\n",
              "      <td>40.725737</td>\n",
              "      <td>CRD</td>\n",
              "      <td>7.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186564</th>\n",
              "      <td>VTS</td>\n",
              "      <td>2014-02-07 00:55:00</td>\n",
              "      <td>2014-02-07 01:15:00</td>\n",
              "      <td>1</td>\n",
              "      <td>4.26</td>\n",
              "      <td>-73.990247</td>\n",
              "      <td>40.737442</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-73.991287</td>\n",
              "      <td>40.692535</td>\n",
              "      <td>CRD</td>\n",
              "      <td>17.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186565</th>\n",
              "      <td>VTS</td>\n",
              "      <td>2014-02-03 18:08:00</td>\n",
              "      <td>2014-02-03 19:29:00</td>\n",
              "      <td>1</td>\n",
              "      <td>57.84</td>\n",
              "      <td>-73.789527</td>\n",
              "      <td>40.645007</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-73.776505</td>\n",
              "      <td>40.740790</td>\n",
              "      <td>CSH</td>\n",
              "      <td>153.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.08</td>\n",
              "      <td>160.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186566</th>\n",
              "      <td>VTS</td>\n",
              "      <td>2014-02-07 00:58:00</td>\n",
              "      <td>2014-02-07 01:12:00</td>\n",
              "      <td>1</td>\n",
              "      <td>3.40</td>\n",
              "      <td>-73.983495</td>\n",
              "      <td>40.694153</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-74.005953</td>\n",
              "      <td>40.710922</td>\n",
              "      <td>CRD</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>17.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186567</th>\n",
              "      <td>VTS</td>\n",
              "      <td>2014-02-07 01:05:00</td>\n",
              "      <td>2014-02-07 01:15:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2.25</td>\n",
              "      <td>-73.987732</td>\n",
              "      <td>40.721013</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-73.972407</td>\n",
              "      <td>40.747463</td>\n",
              "      <td>CSH</td>\n",
              "      <td>9.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14999999 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       vendor_id      pickup_datetime  ... tolls_amount  total_amount\n",
              "0            CMT  2014-01-09 20:45:25  ...         0.00          8.90\n",
              "1            CMT  2014-01-09 20:46:12  ...         0.00         11.40\n",
              "2            CMT  2014-01-09 20:44:47  ...         0.00         14.00\n",
              "3            CMT  2014-01-09 20:44:57  ...         0.00         10.20\n",
              "4            CMT  2014-01-09 20:47:09  ...         0.00          8.75\n",
              "...          ...                  ...  ...          ...           ...\n",
              "186563       VTS  2014-02-03 19:28:00  ...         0.00         10.00\n",
              "186564       VTS  2014-02-07 00:55:00  ...         0.00         20.50\n",
              "186565       VTS  2014-02-03 18:08:00  ...         7.08        160.08\n",
              "186566       VTS  2014-02-07 00:58:00  ...         0.00         17.90\n",
              "186567       VTS  2014-02-07 01:05:00  ...         0.00         10.50\n",
              "\n",
              "[14999999 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaJryrMA_DhF"
      },
      "source": [
        "Using Dask the total processing time is similar, but since Dask only loads a chunk of the Dataframe into memory and the rest is kept on disk until processed, Dask allows me loading large files even though my machine is low on memory."
      ],
      "id": "jaJryrMA_DhF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e743d082"
      },
      "source": [
        "### Using tf.data API (chosen option)"
      ],
      "id": "e743d082"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzutwbVAQSqr"
      },
      "source": [
        "The Dataset API allows you to build an asynchronous, highly optimized data pipeline. A Tensorflow dataset object is a Python iterable and when operations are executed, this object works as a generator by loading only parts of the dataset into memory."
      ],
      "id": "pzutwbVAQSqr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7LaUZ3Ibxib"
      },
      "source": [
        "tf.data.experimental.make_csv_dataset reads CSV files into a dataset, where each element of the dataset is a (features, labels) tuple that corresponds to a batch of CSV rows"
      ],
      "id": "b7LaUZ3Ibxib"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upD3KoellxPo"
      },
      "source": [
        "# Data Ingestion"
      ],
      "id": "upD3KoellxPo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uor378reiPM_"
      },
      "source": [
        "Normal reading process of the file with tf.data"
      ],
      "id": "uor378reiPM_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a764b04f"
      },
      "source": [
        "\n",
        "nyc_taxi_data_ds = tf.data.experimental.make_csv_dataset(\n",
        "    file_path,\n",
        "    batch_size=10, \n",
        "    label_name=None,\n",
        "    num_epochs=1,\n",
        "    shuffle= False,\n",
        "    ignore_errors=True,)"
      ],
      "id": "a764b04f",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXdR_C3Bc0yp"
      },
      "source": [
        "\n",
        "viewing one batch"
      ],
      "id": "OXdR_C3Bc0yp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG0EL51JchRo",
        "outputId": "3a46f7c7-d603-480f-fe6e-ffa8899ee771"
      },
      "source": [
        "%%time\n",
        "next(nyc_taxi_data_ds.__iter__())"
      ],
      "id": "tG0EL51JchRo",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 41 ms, sys: 12 ms, total: 53.1 ms\n",
            "Wall time: 231 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('vendor_id', <tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "              array([b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT'], dtype=object)>),\n",
              "             ('pickup_datetime', <tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "              array([b'2014-01-09 20:45:25', b'2014-01-09 20:46:12',\n",
              "                     b'2014-01-09 20:44:47', b'2014-01-09 20:44:57',\n",
              "                     b'2014-01-09 20:47:09', b'2014-01-09 20:45:07',\n",
              "                     b'2014-01-09 20:44:04', b'2014-01-09 20:43:23',\n",
              "                     b'2014-01-09 20:43:04', b'2014-01-09 20:50:23'], dtype=object)>),\n",
              "             ('dropoff_datetime', <tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "              array([b'2014-01-09 20:52:31', b'2014-01-09 20:55:12',\n",
              "                     b'2014-01-09 20:59:46', b'2014-01-09 20:51:40',\n",
              "                     b'2014-01-09 20:53:32', b'2014-01-09 20:51:01',\n",
              "                     b'2014-01-09 21:05:45', b'2014-01-09 20:52:07',\n",
              "                     b'2014-01-09 20:54:29', b'2014-01-09 20:58:10'], dtype=object)>),\n",
              "             ('passenger_count',\n",
              "              <tf.Tensor: shape=(10,), dtype=int32, numpy=array([1, 1, 2, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>),\n",
              "             ('trip_distance',\n",
              "              <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0.7, 1.4, 2.3, 1.7, 0.9, 0.9, 3.6, 2.1, 3.4, 2.3], dtype=float32)>),\n",
              "             ('pickup_longitude',\n",
              "              <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "              array([-73.99477 , -73.98239 , -73.98857 , -73.96021 , -73.99537 ,\n",
              "                     -73.98381 , -73.98414 , -73.979904, -73.98115 , -73.95519 ],\n",
              "                    dtype=float32)>),\n",
              "             ('pickup_latitude', <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "              array([40.736828, 40.77338 , 40.739407, 40.770466, 40.717247, 40.749657,\n",
              "                     40.72632 , 40.74585 , 40.75892 , 40.76547 ], dtype=float32)>),\n",
              "             ('rate_code',\n",
              "              <tf.Tensor: shape=(10,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>),\n",
              "             ('store_and_fwd_flag',\n",
              "              <tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "              array([b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N'],\n",
              "                    dtype=object)>),\n",
              "             ('dropoff_longitude',\n",
              "              <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "              array([-73.98222 , -73.96045 , -73.986626, -73.979866, -73.98437 ,\n",
              "                     -73.989746, -73.96287 , -73.95909 , -73.94251 , -73.97903 ],\n",
              "                    dtype=float32)>),\n",
              "             ('dropoff_latitude',\n",
              "              <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "              array([40.73179 , 40.763996, 40.765217, 40.77705 , 40.720524, 40.756577,\n",
              "                     40.758442, 40.77364 , 40.785976, 40.740578], dtype=float32)>),\n",
              "             ('payment_type', <tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "              array([b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD'], dtype=object)>),\n",
              "             ('fare_amount', <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "              array([ 6.5,  8.5, 11.5,  7.5,  6. ,  6. , 16.5,  9. , 12. ,  9. ],\n",
              "                    dtype=float32)>),\n",
              "             ('surcharge',\n",
              "              <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=float32)>),\n",
              "             ('mta_tax',\n",
              "              <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=float32)>),\n",
              "             ('tip_amount', <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "              array([1.4 , 1.9 , 1.5 , 1.7 , 1.75, 1.4 , 5.25, 2.  , 2.6 , 1.  ],\n",
              "                    dtype=float32)>),\n",
              "             ('tolls_amount',\n",
              "              <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>),\n",
              "             ('total_amount', <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "              array([ 8.9 , 11.4 , 14.  , 10.2 ,  8.75,  8.4 , 22.75, 12.  , 15.6 ,\n",
              "                     11.  ], dtype=float32)>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHIC3hrKjnJn"
      },
      "source": [
        "The total processing time is similar to Dask, but tf.data takes less memory and allows me to crate a full data pipeline, tha's why it was chosen"
      ],
      "id": "bHIC3hrKjnJn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0cTzwj-MlMi",
        "outputId": "22cc05cf-2d11-42c9-bd8f-1d5a7b109903"
      },
      "source": [
        "column_names = list(nyc_taxi_data_ds.element_spec.keys())\n",
        "column_names"
      ],
      "id": "u0cTzwj-MlMi",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vendor_id',\n",
              " 'pickup_datetime',\n",
              " 'dropoff_datetime',\n",
              " 'passenger_count',\n",
              " 'trip_distance',\n",
              " 'pickup_longitude',\n",
              " 'pickup_latitude',\n",
              " 'rate_code',\n",
              " 'store_and_fwd_flag',\n",
              " 'dropoff_longitude',\n",
              " 'dropoff_latitude',\n",
              " 'payment_type',\n",
              " 'fare_amount',\n",
              " 'surcharge',\n",
              " 'mta_tax',\n",
              " 'tip_amount',\n",
              " 'tolls_amount',\n",
              " 'total_amount']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y7ywoKuy33K",
        "outputId": "57578e0d-6ac4-4a56-87d2-a83982b76df5"
      },
      "source": [
        "%%writefile utils.py\n",
        "import re\n",
        "import yaml\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from csv import reader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def read_config_file(filepath):\n",
        "    with open(filepath, 'r') as stream:\n",
        "        try:\n",
        "            return yaml.safe_load(stream)\n",
        "        except yaml.YAMLError as exc:\n",
        "            logging.error(exc)\n",
        "\n",
        "\n",
        "def get_csv_header(file_path):\n",
        "    with open(file_path, 'r') as read_obj:\n",
        "        csv_reader = reader(read_obj)\n",
        "        header = next(iter(csv_reader))\n",
        "    return header\n",
        "\n",
        "\n",
        "def replacer(string, char):\n",
        "    pattern = char + '{2,}'\n",
        "    string = re.sub(pattern, char, string) \n",
        "    return string\n",
        "\n",
        "def header_cleaning(columns):\n",
        "    columns = map(lambda x: x.lower(), columns)\n",
        "    columns = map(lambda x: re.sub('[^\\w]', '_', x), columns)\n",
        "    columns = map(lambda x: x.strip('_'), columns)\n",
        "    columns = map(lambda x: replacer(x,'_'), columns)\n",
        "    return list(columns)\n",
        "\n",
        "def validate(file_path,table_config):\n",
        "\n",
        "    column_names = get_csv_header(file_path)\n",
        "    #column_names = list(dataset.element_spec.keys())\n",
        "    column_names = header_cleaning(column_names)\n",
        "    column_names.sort()\n",
        "    \n",
        "    column_expected = table_config['columns']\n",
        "    sort_expected_column=column_expected.copy()\n",
        "    sort_expected_column.sort()\n",
        "    \n",
        "    if len(column_names) == len(column_expected) and column_names == sort_expected_column:\n",
        "        print(\"validation passed\")\n",
        "        val_dataset = tf.data.experimental.make_csv_dataset(\n",
        "            file_path,\n",
        "            header=True,\n",
        "            column_names=column_expected,\n",
        "            batch_size=100,\n",
        "            field_delim=table_config['inbound_delimiter'], \n",
        "            label_name=None,\n",
        "            num_epochs=1,\n",
        "            shuffle=False,\n",
        "            ignore_errors=True,)\n",
        "        \n",
        "        return val_dataset\n",
        "\n",
        "    else: \n",
        "        print(\"\\ncolumn name and column length validation failed\")\n",
        "        mismatched_columns_file = list(set(column_names ).difference(sort_expected_column))\n",
        "        if len(mismatched_columns_file)!=0:\n",
        "            print(\"\\nFollowing File columns are not in the YAML file\",mismatched_columns_file) \n",
        "        missing_YAML_file = list(set(sort_expected_column).difference(column_names))\n",
        "        if len(missing_YAML_file)!=0:\n",
        "            print(\"\\nFollowing YAML columns are not in the file uploaded\",missing_YAML_file)\n",
        "        \n",
        "        return None\n",
        "\n",
        "\n",
        "def to_string(x,delimiter):\n",
        "    \n",
        "    if isinstance(x, bytes):\n",
        "        return x.decode() + delimiter\n",
        "    else:\n",
        "        return str(x) + delimiter\n",
        "\n",
        "\n",
        "def count(dataset):\n",
        "    s=0\n",
        "    for batch in dataset:\n",
        "        for key,values in batch.items(): \n",
        "            s+=len(values)\n",
        "            break\n",
        "    return s\n",
        "\n",
        "\n",
        "def batch_to_write(batch):\n",
        "    keys = []\n",
        "    for n,(key,value) in enumerate(batch.items()):\n",
        "        keys.append(key)\n",
        "\n",
        "        if n != 0:\n",
        "            values=np.vstack((values,value.numpy()))\n",
        "        \n",
        "        else:\n",
        "            values = value.numpy()\n",
        "\n",
        "    return keys,values.T \n",
        "\n",
        "\n",
        "def write_txt(txt_file,batch,delimiter):\n",
        "\n",
        "    header, values = batch_to_write(batch)\n",
        "\n",
        "    with open(txt_file,'a') as file:\n",
        "        n_cols = len(header)\n",
        "        delimiter=list(delimiter*(n_cols-1))\n",
        "        delimiter.append('\\n')\n",
        "                \n",
        "        if os.path.getsize(txt_file)==0:\n",
        "            file.writelines(map(to_string,header,delimiter))\n",
        "                    \n",
        "        for row in values:\n",
        "            line=list(map(to_string,list(row),delimiter))      \n",
        "            file.writelines(line)\n",
        "                    \n",
        "    return n_cols\n",
        "\n"
      ],
      "id": "9Y7ywoKuy33K",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do5X-DG6UMs0"
      },
      "source": [
        "Write YAML file"
      ],
      "id": "do5X-DG6UMs0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFhZZsDBUFL3",
        "outputId": "51bab909-2137-467b-a3e8-8a9b988704c5"
      },
      "source": [
        "%%writefile config.yaml\n",
        "file_type: csv\n",
        "dataset_name: \"2014 New York City Taxi Trips\"\n",
        "file_name: nyc_taxi_data_2014\n",
        "inbound_delimiter: \",\"\n",
        "outbound_delimiter: \"|\"\n",
        "skip_leading_rows: 1\n",
        "columns: \n",
        "    - vendor_id\n",
        "    - pickup_datetime\n",
        "    - dropoff_datetime\n",
        "    - passenger_count\n",
        "    - trip_distance\n",
        "    - pickup_longitude\n",
        "    - pickup_latitude\n",
        "    - rate_code\n",
        "    - store_and_fwd_flag\n",
        "    - dropoff_longitude\n",
        "    - dropoff_latitude\n",
        "    - payment_type\n",
        "    - fare_amount\n",
        "    - surcharge\n",
        "    - mta_tax\n",
        "    - tip_amount\n",
        "    - tolls_amount\n",
        "    - total_amount"
      ],
      "id": "uFhZZsDBUFL3",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3JgZjehmaOD"
      },
      "source": [
        "### Read the file using config file"
      ],
      "id": "e3JgZjehmaOD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM14WhgpTtSP"
      },
      "source": [
        "### Write validated file"
      ],
      "id": "fM14WhgpTtSP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sogfFjnVP99"
      },
      "source": [
        "import utils\n",
        "\n",
        "def data_ingestion(yaml_file,destination_file):\n",
        "\n",
        "    config_data = utils.read_config_file(yaml_file)\n",
        "    file_path = './'+config_data['file_name']+'.'+config_data['file_type']\n",
        "    \n",
        "    print('Configuration Data\\n',config_data)\n",
        "    \n",
        "    dataset = utils.validate(file_path,config_data)\n",
        "\n",
        "    if dataset != None:\n",
        "\n",
        "        total_rows=utils.count(dataset)\n",
        "\n",
        "        for batch in dataset:\n",
        "            total_cols = utils.write_txt(destination_file,batch,config_data['outbound_delimiter'])\n",
        "        \n",
        "        file_size = (os.path.getsize(destination_file))/1e+9\n",
        "\n",
        "        summary_out_file = {'Total_rows': total_rows,\n",
        "                            'Total_columns':total_cols,\n",
        "                            'size_file':str(round(file_size, ndigits=2))+\" GB\"}\n",
        "\n",
        "        print('Done! ingestion finished')\n",
        "\n",
        "        return dataset, summary_out_file\n",
        "\n",
        "    else:\n",
        "        return 'Dataset was not created'"
      ],
      "id": "1sogfFjnVP99",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGN1WkMcaj9H"
      },
      "source": [
        "Data ingestion"
      ],
      "id": "rGN1WkMcaj9H"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZRlcBKCaOtI",
        "outputId": "cd3da979-766a-4458-8ef8-ca08758f304e"
      },
      "source": [
        "config_file = 'config.yaml'\n",
        "destination_txt = 'output.txt'\n",
        "dataset, summary_out_file = data_ingestion(config_file,destination_txt)"
      ],
      "id": "0ZRlcBKCaOtI",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuration Data\n",
            " {'file_type': 'csv', 'dataset_name': '2014 New York City Taxi Trips', 'file_name': 'nyc_taxi_data_2014', 'inbound_delimiter': ',', 'outbound_delimiter': '|', 'skip_leading_rows': 1, 'columns': ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount']}\n",
            "validation passed\n",
            "Done! ingestion finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P40E35Tp6SF",
        "outputId": "564f1788-60e4-4fad-9b0e-15e8852f3188"
      },
      "source": [
        "summary_out_file"
      ],
      "id": "9P40E35Tp6SF",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Total_columns': 18, 'Total_rows': 14999999, 'size_file': '2.66 GB'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku_CxDnfdqQE",
        "outputId": "7e50ae0c-0557-4c47-9f6b-ef32a768876a"
      },
      "source": [
        "# dataset batch\n",
        "next(dataset.__iter__())"
      ],
      "id": "Ku_CxDnfdqQE",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('vendor_id', <tf.Tensor: shape=(100,), dtype=string, numpy=\n",
              "              array([b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT', b'CMT',\n",
              "                     b'CMT', b'CMT', b'CMT', b'CMT'], dtype=object)>),\n",
              "             ('pickup_datetime', <tf.Tensor: shape=(100,), dtype=string, numpy=\n",
              "              array([b'2014-01-09 20:45:25', b'2014-01-09 20:46:12',\n",
              "                     b'2014-01-09 20:44:47', b'2014-01-09 20:44:57',\n",
              "                     b'2014-01-09 20:47:09', b'2014-01-09 20:45:07',\n",
              "                     b'2014-01-09 20:44:04', b'2014-01-09 20:43:23',\n",
              "                     b'2014-01-09 20:43:04', b'2014-01-09 20:50:23',\n",
              "                     b'2014-01-09 20:51:36', b'2014-01-09 20:48:04',\n",
              "                     b'2014-01-09 20:47:49', b'2014-01-09 20:48:47',\n",
              "                     b'2014-01-09 20:47:51', b'2014-01-09 20:49:49',\n",
              "                     b'2014-01-09 16:51:35', b'2014-01-09 16:43:29',\n",
              "                     b'2014-01-09 16:46:50', b'2014-01-09 16:47:00',\n",
              "                     b'2014-01-09 16:51:14', b'2014-01-09 16:42:16',\n",
              "                     b'2014-01-09 16:42:05', b'2014-01-09 16:43:37',\n",
              "                     b'2014-01-09 16:46:36', b'2014-01-09 16:49:16',\n",
              "                     b'2014-01-09 16:48:12', b'2014-01-09 16:44:22',\n",
              "                     b'2014-01-09 16:50:42', b'2014-01-09 16:48:14',\n",
              "                     b'2014-01-09 16:48:35', b'2014-01-09 16:48:42',\n",
              "                     b'2014-01-09 16:45:21', b'2014-01-09 16:48:27',\n",
              "                     b'2014-01-09 16:49:33', b'2014-01-09 16:46:38',\n",
              "                     b'2014-01-09 16:44:07', b'2014-01-09 16:48:37',\n",
              "                     b'2014-01-09 16:45:41', b'2014-01-09 16:45:35',\n",
              "                     b'2014-01-09 16:50:33', b'2014-01-09 21:49:26',\n",
              "                     b'2014-01-09 21:47:04', b'2014-01-09 21:49:36',\n",
              "                     b'2014-01-09 21:51:25', b'2014-01-09 21:48:04',\n",
              "                     b'2014-01-09 21:49:31', b'2014-01-09 21:51:48',\n",
              "                     b'2014-01-09 21:44:59', b'2014-01-09 21:45:30',\n",
              "                     b'2014-01-09 21:53:53', b'2014-01-09 21:54:09',\n",
              "                     b'2014-01-09 21:51:23', b'2014-01-09 21:52:51',\n",
              "                     b'2014-01-09 21:56:27', b'2014-01-09 21:53:26',\n",
              "                     b'2014-01-09 21:50:20', b'2014-01-09 21:51:16',\n",
              "                     b'2014-01-09 21:44:48', b'2014-01-09 21:47:23',\n",
              "                     b'2014-01-09 21:44:53', b'2014-01-09 21:48:11',\n",
              "                     b'2014-01-09 21:49:35', b'2014-01-09 21:48:44',\n",
              "                     b'2014-01-09 21:48:46', b'2014-01-09 21:48:01',\n",
              "                     b'2014-01-09 21:52:28', b'2014-01-09 21:48:43',\n",
              "                     b'2014-01-09 21:51:14', b'2014-01-09 21:55:52',\n",
              "                     b'2014-01-09 21:47:06', b'2014-01-09 21:56:31',\n",
              "                     b'2014-01-09 21:50:48', b'2014-01-09 21:53:56',\n",
              "                     b'2014-01-09 21:56:40', b'2014-01-09 21:54:03',\n",
              "                     b'2014-01-09 21:57:26', b'2014-01-09 21:55:21',\n",
              "                     b'2014-01-09 21:52:52', b'2014-01-09 21:54:18',\n",
              "                     b'2014-01-09 21:57:19', b'2014-01-09 21:56:07',\n",
              "                     b'2014-01-09 20:07:02', b'2014-01-09 20:12:06',\n",
              "                     b'2014-01-09 20:05:25', b'2014-01-09 20:09:22',\n",
              "                     b'2014-01-09 20:06:15', b'2014-01-09 20:09:27',\n",
              "                     b'2014-01-09 20:11:13', b'2014-01-09 22:13:28',\n",
              "                     b'2014-01-09 22:19:44', b'2014-01-09 22:14:35',\n",
              "                     b'2014-01-09 22:12:07', b'2014-01-09 18:57:22',\n",
              "                     b'2014-01-09 18:57:27', b'2014-01-09 18:56:36',\n",
              "                     b'2014-01-09 19:07:08', b'2014-01-09 19:04:36',\n",
              "                     b'2014-01-09 19:04:56', b'2014-01-09 19:07:54'], dtype=object)>),\n",
              "             ('dropoff_datetime',\n",
              "              <tf.Tensor: shape=(100,), dtype=string, numpy=\n",
              "              array([b'2014-01-09 20:52:31', b'2014-01-09 20:55:12',\n",
              "                     b'2014-01-09 20:59:46', b'2014-01-09 20:51:40',\n",
              "                     b'2014-01-09 20:53:32', b'2014-01-09 20:51:01',\n",
              "                     b'2014-01-09 21:05:45', b'2014-01-09 20:52:07',\n",
              "                     b'2014-01-09 20:54:29', b'2014-01-09 20:58:10',\n",
              "                     b'2014-01-09 21:15:07', b'2014-01-09 21:01:37',\n",
              "                     b'2014-01-09 20:56:11', b'2014-01-09 20:56:52',\n",
              "                     b'2014-01-09 21:02:31', b'2014-01-09 21:20:38',\n",
              "                     b'2014-01-09 17:00:17', b'2014-01-09 16:59:15',\n",
              "                     b'2014-01-09 16:56:41', b'2014-01-09 17:37:58',\n",
              "                     b'2014-01-09 16:51:34', b'2014-01-09 16:58:42',\n",
              "                     b'2014-01-09 17:30:27', b'2014-01-09 16:56:23',\n",
              "                     b'2014-01-09 16:52:44', b'2014-01-09 17:01:00',\n",
              "                     b'2014-01-09 16:52:09', b'2014-01-09 16:47:59',\n",
              "                     b'2014-01-09 17:10:17', b'2014-01-09 16:57:56',\n",
              "                     b'2014-01-09 16:55:40', b'2014-01-09 17:13:00',\n",
              "                     b'2014-01-09 16:49:01', b'2014-01-09 17:11:35',\n",
              "                     b'2014-01-09 17:09:07', b'2014-01-09 16:57:00',\n",
              "                     b'2014-01-09 17:04:22', b'2014-01-09 16:54:16',\n",
              "                     b'2014-01-09 17:38:15', b'2014-01-09 17:27:44',\n",
              "                     b'2014-01-09 17:26:56', b'2014-01-09 21:55:20',\n",
              "                     b'2014-01-09 21:50:38', b'2014-01-09 21:57:14',\n",
              "                     b'2014-01-09 22:10:02', b'2014-01-09 21:55:46',\n",
              "                     b'2014-01-09 21:51:59', b'2014-01-09 21:55:08',\n",
              "                     b'2014-01-09 22:04:38', b'2014-01-09 22:16:49',\n",
              "                     b'2014-01-09 22:00:45', b'2014-01-09 22:04:06',\n",
              "                     b'2014-01-09 22:25:38', b'2014-01-09 22:00:31',\n",
              "                     b'2014-01-09 22:03:59', b'2014-01-09 22:10:11',\n",
              "                     b'2014-01-09 22:09:37', b'2014-01-09 22:03:26',\n",
              "                     b'2014-01-09 22:13:47', b'2014-01-09 22:05:41',\n",
              "                     b'2014-01-09 22:08:45', b'2014-01-09 21:57:40',\n",
              "                     b'2014-01-09 22:02:19', b'2014-01-09 22:00:51',\n",
              "                     b'2014-01-09 21:54:13', b'2014-01-09 21:59:00',\n",
              "                     b'2014-01-09 22:10:03', b'2014-01-09 21:48:55',\n",
              "                     b'2014-01-09 21:55:03', b'2014-01-09 22:03:10',\n",
              "                     b'2014-01-09 22:11:06', b'2014-01-09 22:14:33',\n",
              "                     b'2014-01-09 22:18:05', b'2014-01-09 22:02:22',\n",
              "                     b'2014-01-09 22:08:58', b'2014-01-09 22:04:22',\n",
              "                     b'2014-01-09 22:06:33', b'2014-01-09 22:03:50',\n",
              "                     b'2014-01-09 22:11:41', b'2014-01-09 22:04:39',\n",
              "                     b'2014-01-09 22:03:42', b'2014-01-09 22:11:03',\n",
              "                     b'2014-01-09 20:21:27', b'2014-01-09 20:15:00',\n",
              "                     b'2014-01-09 20:39:53', b'2014-01-09 20:18:15',\n",
              "                     b'2014-01-09 20:18:54', b'2014-01-09 20:18:19',\n",
              "                     b'2014-01-09 20:16:56', b'2014-01-09 22:35:03',\n",
              "                     b'2014-01-09 22:26:58', b'2014-01-09 22:27:15',\n",
              "                     b'2014-01-09 22:30:43', b'2014-01-09 19:14:23',\n",
              "                     b'2014-01-09 19:15:31', b'2014-01-09 19:40:56',\n",
              "                     b'2014-01-09 19:09:59', b'2014-01-09 19:20:03',\n",
              "                     b'2014-01-09 19:17:54', b'2014-01-09 19:18:33'], dtype=object)>),\n",
              "             ('passenger_count', <tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
              "              array([1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1,\n",
              "                     1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
              "                     1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "                     1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2,\n",
              "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>),\n",
              "             ('trip_distance', <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([ 0.7,  1.4,  2.3,  1.7,  0.9,  0.9,  3.6,  2.1,  3.4,  2.3,  9.5,\n",
              "                      3.3,  1.8,  1.4,  2.6, 11.2,  1.7,  4.7,  1.6, 17.9,  0. ,  2.1,\n",
              "                     14.3,  1.6,  1.3,  1.3,  0.8,  0.6,  3.8,  1.8,  1.2,  2.3,  0.6,\n",
              "                      4.3,  2.6,  2.6,  8.3,  1.9, 16.7,  7.7,  3.1,  1.4,  0.9,  1.7,\n",
              "                      7. ,  1.5,  0.5,  0.6,  5. ,  8.7,  1.5,  1.8,  9.7,  2.6,  1.3,\n",
              "                      4.1,  2.3,  2.5, 13.4,  3.5,  5.6,  1.3,  4.1,  3.2,  0.9,  2.4,\n",
              "                      4.2,  0. ,  0.5,  1.4, 10.7,  3.8,  5.3,  2.9,  5.3,  2.9,  1.9,\n",
              "                      1.5,  4.7,  2.6,  1.6,  3.7,  3.4,  0.4, 13. ,  1.4,  2.1,  2. ,\n",
              "                      1.4,  5.1,  1.7,  3.2,  4.3,  2. ,  2.6, 19.9,  0.4,  1.8,  2.6,\n",
              "                      2.2], dtype=float32)>),\n",
              "             ('pickup_longitude',\n",
              "              <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([-73.99477 , -73.98239 , -73.98857 , -73.96021 , -73.99537 ,\n",
              "                     -73.98381 , -73.98414 , -73.979904, -73.98115 , -73.95519 ,\n",
              "                     -73.88528 , -73.99178 , -73.96571 , -73.97701 , -73.97765 ,\n",
              "                     -73.78827 , -74.0075  , -74.01487 , -73.967674, -73.78173 ,\n",
              "                     -73.79056 , -73.98881 , -74.01369 , -73.970116, -73.99154 ,\n",
              "                     -73.96064 , -73.97973 , -73.97538 , -73.94621 , -73.99216 ,\n",
              "                     -73.990265, -73.98909 , -73.977264, -74.01238 , -73.979546,\n",
              "                     -73.95214 , -73.865555, -73.93414 , -73.989494, -73.96899 ,\n",
              "                     -73.9404  , -74.002426, -73.96047 , -73.98082 , -74.00346 ,\n",
              "                     -73.979416, -73.98953 , -73.97041 , -73.982765, -73.86301 ,\n",
              "                     -73.991135, -74.00266 , -73.97629 , -74.00271 , -73.98804 ,\n",
              "                     -73.991585, -73.97584 , -73.99011 , -74.01271 , -73.967804,\n",
              "                     -74.00832 , -73.969055, -74.002495, -73.98556 , -73.988884,\n",
              "                     -73.978424, -74.00529 , -73.99076 , -73.99239 , -73.982315,\n",
              "                     -73.976685, -73.952194, -73.98041 , -74.00786 , -73.94718 ,\n",
              "                     -73.987785, -73.96775 , -73.98692 , -73.9923  , -73.98196 ,\n",
              "                     -74.00017 , -73.99047 , -73.95413 , -73.98498 , -73.87309 ,\n",
              "                       0.      , -73.98488 , -73.96485 , -73.96551 , -73.99023 ,\n",
              "                     -73.96821 ,   0.      , -73.97802 , -74.0081  , -74.00515 ,\n",
              "                     -73.77674 , -73.98255 , -73.951195, -73.995766, -73.99059 ],\n",
              "                    dtype=float32)>),\n",
              "             ('pickup_latitude',\n",
              "              <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([40.736828, 40.77338 , 40.739407, 40.770466, 40.717247, 40.749657,\n",
              "                     40.72632 , 40.74585 , 40.75892 , 40.76547 , 40.77305 , 40.748913,\n",
              "                     40.758675, 40.75162 , 40.75368 , 40.64754 , 40.72599 , 40.709354,\n",
              "                     40.763107, 40.64473 , 40.643936, 40.751015, 40.709328, 40.75286 ,\n",
              "                     40.73892 , 40.768772, 40.78171 , 40.761055, 40.77299 , 40.731083,\n",
              "                     40.72905 , 40.73667 , 40.743053, 40.706738, 40.73546 , 40.82443 ,\n",
              "                     40.77048 , 40.801945, 40.74045 , 40.765663, 40.751305, 40.734447,\n",
              "                     40.770138, 40.76435 , 40.748905, 40.766495, 40.734318, 40.7595  ,\n",
              "                     40.743565, 40.769184, 40.73973 , 40.751133, 40.75459 , 40.733162,\n",
              "                     40.718685, 40.735313, 40.749046, 40.719048, 40.70247 , 40.76086 ,\n",
              "                     40.729774, 40.7611  , 40.75019 , 40.731586, 40.73691 , 40.736843,\n",
              "                     40.742397, 40.72059 , 40.725086, 40.77167 , 40.75506 , 40.781406,\n",
              "                     40.753944, 40.742874, 40.776073, 40.7601  , 40.756126, 40.733265,\n",
              "                     40.729065, 40.771244, 40.73302 , 40.73485 , 40.765564, 40.763725,\n",
              "                     40.773952,  0.      , 40.7405  , 40.755623, 40.76128 , 40.737053,\n",
              "                     40.75537 ,  0.      , 40.783504, 40.746105, 40.74796 , 40.645195,\n",
              "                     40.742622, 40.77655 , 40.744007, 40.702663], dtype=float32)>),\n",
              "             ('rate_code', <tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
              "              array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1,\n",
              "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
              "                     1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "                     1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "                     1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1], dtype=int32)>),\n",
              "             ('store_and_fwd_flag',\n",
              "              <tf.Tensor: shape=(100,), dtype=string, numpy=\n",
              "              array([b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N',\n",
              "                     b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'Y',\n",
              "                     b'N', b'N', b'N', b'N', b'Y', b'N', b'N', b'N', b'N', b'N', b'N',\n",
              "                     b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N',\n",
              "                     b'N', b'N', b'N', b'N', b'Y', b'N', b'N', b'N', b'Y', b'N', b'N',\n",
              "                     b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N',\n",
              "                     b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N',\n",
              "                     b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N',\n",
              "                     b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N', b'N',\n",
              "                     b'N'], dtype=object)>),\n",
              "             ('dropoff_longitude',\n",
              "              <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([-73.98222 , -73.96045 , -73.986626, -73.979866, -73.98437 ,\n",
              "                     -73.989746, -73.96287 , -73.95909 , -73.94251 , -73.97903 ,\n",
              "                     -73.98088 , -73.98836 , -73.98406 , -73.98264 , -73.95225 ,\n",
              "                     -73.94923 , -73.98818 , -73.986084, -73.95259 , -73.97861 ,\n",
              "                     -73.79056 , -73.996   , -73.88567 , -73.95414 , -73.99474 ,\n",
              "                     -73.97668 , -73.97665 , -73.979164, -73.99402 , -73.97538 ,\n",
              "                     -73.995735, -73.98574 , -73.9854  , -73.951965, -73.97149 ,\n",
              "                     -73.9683  , -73.95818 , -73.95259 , -73.78361 , -73.99564 ,\n",
              "                     -73.96446 , -73.98608 , -73.955055, -73.991234, -73.99672 ,\n",
              "                     -73.99829 , -73.983345, -73.974724, -73.954254, -73.98771 ,\n",
              "                     -74.00111 , -74.00266 , -74.05793 , -74.00271 , -74.00559 ,\n",
              "                     -73.9581  , -73.97199 , -73.993256, -73.86003 , -74.00471 ,\n",
              "                     -73.95404 , -73.98873 , -73.95592 , -73.98436 , -73.98997 ,\n",
              "                     -73.981285, -73.96011 , -73.99076 , -73.98853 , -73.967674,\n",
              "                     -73.9763  , -73.98608 , -73.99174 , -73.98528 , -73.98667 ,\n",
              "                     -73.95566 , -73.98775 , -74.00568 , -73.98142 , -73.95292 ,\n",
              "                     -73.97907 , -73.95788 , -73.98487 , -73.98228 , -73.93895 ,\n",
              "                       0.      , -74.00783 , -73.94746 , -73.957924, -73.94044 ,\n",
              "                     -73.951836,   0.      , -73.989525, -73.97835 , -73.9937  ,\n",
              "                     -74.01026 , -73.98885 , -73.97    , -74.01043 , -74.01418 ],\n",
              "                    dtype=float32)>),\n",
              "             ('dropoff_latitude',\n",
              "              <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([40.73179 , 40.763996, 40.765217, 40.77705 , 40.720524, 40.756577,\n",
              "                     40.758442, 40.77364 , 40.785976, 40.740578, 40.777386, 40.714207,\n",
              "                     40.73745 , 40.766575, 40.777676, 40.6527  , 40.734585, 40.759083,\n",
              "                     40.778187, 40.76182 , 40.643932, 40.726658, 40.773193, 40.76408 ,\n",
              "                     40.724133, 40.76534 , 40.79089 , 40.753876, 40.75025 , 40.75045 ,\n",
              "                     40.741028, 40.7627  , 40.739822, 40.71176 , 40.76534 , 40.79196 ,\n",
              "                     40.800846, 40.779057, 40.643684, 40.697605, 40.764385, 40.74022 ,\n",
              "                     40.78175 , 40.750717, 40.663364, 40.756863, 40.734478, 40.755363,\n",
              "                     40.80306 , 40.770283, 40.721733, 40.751133, 40.742813, 40.733162,\n",
              "                     40.71786 , 40.717484, 40.755997, 40.74653 , 40.720963, 40.738316,\n",
              "                     40.78434 , 40.762424, 40.77386 , 40.691517, 40.74707 , 40.762653,\n",
              "                     40.77047 , 40.7206  , 40.719917, 40.75821 , 40.657356, 40.734867,\n",
              "                     40.69131 , 40.778812, 40.720917, 40.77963 , 40.73763 , 40.745605,\n",
              "                     40.78272 , 40.78669 , 40.737232, 40.71414 , 40.749214, 40.768948,\n",
              "                     40.663902,  0.      , 40.716705, 40.78195 , 40.77903 , 40.694233,\n",
              "                     40.773468,  0.      , 40.73607 , 40.729504, 40.721516, 40.717533,\n",
              "                     40.741207, 40.758442, 40.7191  , 40.716972], dtype=float32)>),\n",
              "             ('payment_type', <tf.Tensor: shape=(100,), dtype=string, numpy=\n",
              "              array([b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD', b'CRD',\n",
              "                     b'CRD', b'CRD', b'CRD', b'CRD'], dtype=object)>),\n",
              "             ('fare_amount', <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([ 6.5,  8.5, 11.5,  7.5,  6. ,  6. , 16.5,  9. , 12. ,  9. , 28.5,\n",
              "                     12.5,  8.5,  7.5, 12.5, 35.5,  8.5, 16. ,  9. , 52. , 52. , 12.5,\n",
              "                     45.5, 10. ,  6.5,  9. ,  5. ,  4.5, 15.5,  9. ,  6.5, 16. ,  4.5,\n",
              "                     18. , 14. , 10.5, 25. ,  7.5, 52. , 30.5, 21.5,  6.5,  5. ,  8. ,\n",
              "                     21.5,  7.5,  4. ,  4.5, 19. , 29.5,  7. ,  9. , 61. , 10. ,  7.5,\n",
              "                     15.5, 13.5, 11. , 38. , 14.5, 20. ,  8. , 13.5, 12.5,  6. , 10.5,\n",
              "                     16. , 52. ,  4.5,  7. , 31.5, 15. , 21. , 10.5, 17. , 10.5,  9. ,\n",
              "                      8. , 17. , 10.5,  7. , 14.5, 13.5,  4. , 38.5,  7.5, 10.5,  9. ,\n",
              "                      7. , 20. ,  7.5, 12.5, 16. , 12.5, 13.5, 52. ,  4. , 11. , 11. ,\n",
              "                     10. ], dtype=float32)>),\n",
              "             ('surcharge', <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 1. , 1. , 1. , 0. , 0. , 1. , 1. , 1. , 1. , 1. ,\n",
              "                     1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0. ,\n",
              "                     1. , 1. , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0. , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0. , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 0.5, 1. , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 1. , 1. , 0. , 1. , 1. , 1. , 1. ], dtype=float32)>),\n",
              "             ('mta_tax', <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0. , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
              "                     0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=float32)>),\n",
              "             ('tip_amount', <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([ 1.4 ,  1.9 ,  1.5 ,  1.7 ,  1.75,  1.4 ,  5.25,  2.  ,  2.6 ,\n",
              "                      1.  ,  6.96,  4.05,  1.9 ,  1.7 ,  1.  ,  0.  ,  2.  ,  4.  ,\n",
              "                      2.1 , 11.56, 14.45,  3.5 ,  4.  ,  1.  ,  2.  ,  2.1 ,  0.75,\n",
              "                      1.2 ,  3.4 ,  3.15,  1.6 ,  1.  ,  1.  ,  1.5 ,  1.5 ,  2.4 ,\n",
              "                      4.  ,  1.8 ,  5.55,  6.4 ,  2.  ,  1.5 ,  1.2 ,  1.8 ,  5.56,\n",
              "                      1.  ,  1.  ,  1.1 ,  4.  ,  5.  ,  1.6 ,  2.5 ,  0.  ,  2.2 ,\n",
              "                      1.08,  2.  ,  2.9 ,  1.  ,  6.  ,  3.87,  5.25,  1.  ,  2.9 ,\n",
              "                      2.7 ,  1.4 ,  2.3 ,  3.4 , 10.5 ,  1.  ,  1.6 ,  5.  ,  2.  ,\n",
              "                      4.4 ,  2.3 ,  3.6 ,  2.3 ,  2.  ,  1.8 ,  3.  ,  3.45,  1.6 ,\n",
              "                      3.1 ,  3.  ,  1.25,  1.  ,  1.  ,  1.25,  1.5 ,  1.6 ,  4.2 ,\n",
              "                      1.7 ,  4.05,  3.4 ,  2.8 ,  3.  , 10.5 ,  1.1 ,  3.  ,  0.2 ,\n",
              "                      2.3 ], dtype=float32)>),\n",
              "             ('tolls_amount', <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 5.33,\n",
              "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 5.33, 5.33, 0.  ,\n",
              "                     5.33, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "                     0.  , 0.  , 0.  , 5.33, 0.  , 5.33, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "                     5.33, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 9.  , 0.  , 0.  ,\n",
              "                     0.  , 0.  , 0.  , 5.33, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "                     0.  ], dtype=float32)>),\n",
              "             ('total_amount', <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "              array([ 8.9 , 11.4 , 14.  , 10.2 ,  8.75,  8.4 , 22.75, 12.  , 15.6 ,\n",
              "                     11.  , 41.79, 17.55, 11.4 , 10.2 , 14.5 , 36.5 , 12.  , 21.5 ,\n",
              "                     12.6 , 69.39, 72.28, 17.5 , 56.33, 12.5 , 10.  , 12.6 ,  7.25,\n",
              "                      7.2 , 20.4 , 13.65,  9.6 , 18.5 ,  7.  , 21.  , 17.  , 14.4 ,\n",
              "                     35.83, 10.8 , 63.38, 38.4 , 25.  ,  9.  ,  7.2 , 10.8 , 33.39,\n",
              "                      9.5 ,  6.  ,  6.6 , 24.  , 35.5 ,  9.6 , 12.5 , 70.  , 13.2 ,\n",
              "                      9.58, 18.5 , 17.4 , 13.  , 50.33, 19.37, 26.25, 10.  , 17.4 ,\n",
              "                     16.2 ,  8.4 , 13.8 , 20.4 , 63.  ,  6.5 ,  9.6 , 37.5 , 18.  ,\n",
              "                     26.4 , 13.8 , 21.6 , 13.8 , 12.  , 10.8 , 21.  , 14.95,  9.6 ,\n",
              "                     18.6 , 18.  ,  6.25, 40.5 ,  9.5 , 12.75, 11.5 ,  9.6 , 25.2 ,\n",
              "                     10.2 , 17.55, 20.4 , 16.8 , 18.  , 63.  ,  6.6 , 15.5 , 12.7 ,\n",
              "                     13.8 ], dtype=float32)>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdE-Mvx7zW6"
      },
      "source": [
        "### Testing a file that does not match "
      ],
      "id": "pcdE-Mvx7zW6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgyxqzaGGWlp"
      },
      "source": [
        "Create a new test configuration file by renaming only the csv file"
      ],
      "id": "UgyxqzaGGWlp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzHSUQegGB-K",
        "outputId": "e64cff88-90ad-47f4-f473-28574a90e3dc"
      },
      "source": [
        "%%writefile test_config.yaml\n",
        "\n",
        "file_type: csv\n",
        "dataset_name: \"Test dataset\"\n",
        "file_name: test_file\n",
        "inbound_delimiter: \",\"\n",
        "outbound_delimiter: \"|\"\n",
        "skip_leading_rows: 1\n",
        "columns: \n",
        "    - vendor_id\n",
        "    - pickup_datetime\n",
        "    - dropoff_datetime\n",
        "    - passenger_count\n",
        "    - trip_distance\n",
        "    - pickup_longitude\n",
        "    - pickup_latitude\n",
        "    - rate_code\n",
        "    - store_and_fwd_flag\n",
        "    - dropoff_longitude\n",
        "    - dropoff_latitude\n",
        "    - payment_type\n",
        "    - fare_amount\n",
        "    - surcharge\n",
        "    - mta_tax\n",
        "    - tip_amount\n",
        "    - tolls_amount\n",
        "    - total_amount"
      ],
      "id": "CzHSUQegGB-K",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sFM4YxiIjDa"
      },
      "source": [
        "Create a new csv file with a different header"
      ],
      "id": "1sFM4YxiIjDa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KWRpk1595FP"
      },
      "source": [
        "import csv  \n",
        "\n",
        "header = utils.get_csv_header(file_path)\n",
        "header.append('New field')\n",
        "\n",
        "with open('test_file.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    # write the header\n",
        "    writer.writerow(header)\n"
      ],
      "id": "2KWRpk1595FP",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJLoB4TmI1Ub"
      },
      "source": [
        "Validate new file"
      ],
      "id": "YJLoB4TmI1Ub"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm5ztP47CWfI",
        "outputId": "2c336f0d-d5fa-46c8-f90c-a0f710441a90"
      },
      "source": [
        "test_file = './test.txt'\n",
        "test_dataset = data_ingestion('./test_config.yaml',test_file)"
      ],
      "id": "Hm5ztP47CWfI",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuration Data\n",
            " {'file_type': 'csv', 'dataset_name': 'Test dataset', 'file_name': 'test_file', 'inbound_delimiter': ',', 'outbound_delimiter': '|', 'skip_leading_rows': 1, 'columns': ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount']}\n",
            "\n",
            "column name and column length validation failed\n",
            "\n",
            "Following File columns are not in the YAML file ['new_field']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}